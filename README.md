# Methuselah‑prediction

This repository provides a minimalist pipeline for predicting
biological outcomes from publicly available *Saccharomyces cerevisiae*
(yeast) datasets.  The current demo uses the [UCI Yeast
dataset](https://archive.ics.uci.edu/dataset/110/yeast), which
contains eight continuous features describing protein sequences and a
categorical label corresponding to the cellular localisation site.

The long‑term goal of this project is to predict chronological
lifespan (CLS) or stress‑survival outcomes under nutrient
perturbations using simple, reproducible machine‑learning baselines.
This initial implementation provides infrastructure to download or
store raw data, preprocess it into a clean CSV, train a classifier,
evaluate metrics and compute feature importances.

## Quick start

1. **Install dependencies** (preferably in a virtual environment):

   ```bash
   pip install -r requirements.txt
   ```

2. **Prepare the data** (reads `data/raw/yeast.data` and produces
   `data/processed/processed.csv`):

   ```bash
   python src/prepare_data.py --config configs/base.yaml
   ```

3. **Train a model** (uses the processed CSV and writes metrics
   and a model into `results/`):

   ```bash
   python src/train.py --config configs/base.yaml
   ```

4. **Evaluate on the full dataset** (optional):

   ```bash
   python src/evaluate.py --config configs/base.yaml
   ```

5. **Interpret the model** via permutation feature importance (writes
   `feature_importance.png` into the results directory):

   ```bash
   python src/interpret.py --config configs/base.yaml
   ```

## Directory structure

```
├── data/
│   ├── raw/               # raw dataset files (yeast.data, yeast.names)
│   └── processed/         # processed CSV generated by prepare_data.py
├── configs/
│   └── base.yaml          # configuration describing paths, split fractions and model hyperparameters
├── src/
│   ├── prepare_data.py    # preprocess raw data
│   ├── train.py           # train baseline model and report metrics
│   ├── evaluate.py        # evaluate model on full dataset
│   └── interpret.py       # permutation feature importance
├── results/               # metrics and trained models will be written here
├── requirements.txt       # Python dependencies
└── README.md              # this document
```

## License

The UCI Yeast dataset is released under a [Creative Commons
Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/)
license.  Please cite Kenta Nakai’s paper when using this data:

> Nakai, K. (1991). Yeast [Dataset]. UCI Machine Learning
> Repository. https://doi.org/10.24432/C5KG68

The code in this repository is licensed under the MIT license; see
`LICENSE` for details.

## Experimental cross‑validation and algorithm comparison

Recent advances in aging research suggest that model choice and
hyperparameter tuning greatly influence predictive performance.  In
particular, elastic‑net penalised logistic regression often performs
well relative to other algorithms when features are highly
correlated, while nonlinear models like support‑vector machines and
gradient‑boosted trees can capture complex patterns【348516617573011†L170-L198】.
Supervised learning reviews also emphasise that feature selection,
stratified cross‑validation and multi‑metric evaluation (e.g., AUROC,
AUPRC, accuracy and Brier score) are essential for robust
benchmarking【257543399768406†L260-L329】.

To explore these recommendations, the script `src/train_experiment.py`
implements a configurable experimental pipeline.  Given a processed
dataset and a YAML configuration file, it performs stratified
cross‑validation over multiple algorithms, searches hyperparameters
via grid search using macro‑averaged one‑vs‑rest ROC AUC as the
selection criterion and reports averaged metrics across folds.

Run the experiment as follows:

```bash
cd project
python src/train_experiment.py --config configs/base.yaml
```

The `experiment` section of the configuration controls the design:

- `cv_folds`: number of stratified folds (default 5).
- `algorithms`: list of model names to evaluate (options: `elastic_net_logreg`, `svm`, `xgboost`).
- `param_grids`: optional hyperparameter grids overriding the defaults.

After running, results are written to `results/experiment_results.json` and
the best model (based on AUROC) is saved to `results/best_model.joblib`.