# Methuselah‑prediction

This repository provides a minimalist pipeline for predicting
biological outcomes from publicly available *Saccharomyces cerevisiae*
(yeast) datasets.  The current demo uses the [UCI Yeast
dataset](https://archive.ics.uci.edu/dataset/110/yeast), which
contains eight continuous features describing protein sequences and a
categorical label corresponding to the cellular localisation site.

The long‑term goal of this project is to predict chronological
lifespan (CLS) or stress‑survival outcomes under nutrient
perturbations using simple, reproducible machine‑learning baselines.
This initial implementation provides infrastructure to download or
store raw data, preprocess it into a clean CSV, train a classifier,
evaluate metrics and compute feature importances.

## Quick start

1. **Install dependencies** (preferably in a virtual environment):

   ```bash
   pip install -r requirements.txt
   ```

2. **Prepare the data** (reads `data/raw/yeast.data` and produces
   `data/processed/processed.csv`):

   ```bash
   python src/prepare_data.py --config configs/base.yaml
   ```

3. **Train a model** (uses the processed CSV and writes metrics
   and a model into `results/`):

   ```bash
   python src/train.py --config configs/base.yaml
   ```

4. **Evaluate on the full dataset** (optional):

   ```bash
   python src/evaluate.py --config configs/base.yaml
   ```

5. **Interpret the model**.  Two complementary interpretability
   methods are provided:

   * **Permutation importance**, which measures the drop in
     performance when each feature is shuffled, producing a
     bar chart (`feature_importance.png`).  Run:

     ```bash
     python src/interpret.py --config configs/base.yaml
     ```

   * **SHAP values**, which attribute individual predictions to
     feature contributions.  The script `interpret_shap.py` writes a
     summary plot (`shap_summary.png`) if the optional `shap` package
     is installed:

     ```bash
     python src/interpret_shap.py --config configs/base.yaml
     ```

     You may need to install the extra dependency with
     `pip install shap`.

## Directory structure

```
├── data/
│   ├── raw/               # raw dataset files (yeast.data, yeast.names)
│   └── processed/         # processed CSV generated by prepare_data.py
├── configs/
│   └── base.yaml          # configuration describing paths, split fractions and model hyperparameters
├── src/
│   ├── prepare_data.py    # preprocess raw data
│   ├── train.py           # train baseline model and report metrics
│   ├── evaluate.py        # evaluate model on full dataset
│   ├── interpret.py       # permutation feature importance
│   ├── train_experiment.py # cross‑validated algorithm comparison and hyperparameter tuning
│   ├── merge_datasets.py  # merge multiple processed datasets into one
│   ├── visualize_experiment.py  # plot bar chart of experiment metrics
│   ├── feature_selection.py  # univariate feature selection diagnostic
│   ├── runner.py       # orchestrate full pipeline (preprocess, merge, train, select features and visualise)
│   ├── stacking_ensemble.py  # train a stacking ensemble combining multiple models
│   ├── train_nested_cv.py  # nested cross‑validation for unbiased hyperparameter selection
│   ├── eda.py             # exploratory data analysis (stats, correlation, UMAP)
│   ├── interpret_shap.py  # model interpretation using SHAP values
│   ├── automl.py          # AutoML search via FLAML
│   ├── advanced_models.py # skeletons for transformers, survival and graph models
│   └── __init__.py        # allow tests to import src as a package
├── results/               # metrics and trained models will be written here
├── requirements.txt       # Python dependencies
└── README.md              # this document
```

## License

The UCI Yeast dataset is released under a [Creative Commons
Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/)
license.  Please cite Kenta Nakai’s paper when using this data:

> Nakai, K. (1991). Yeast [Dataset]. UCI Machine Learning
> Repository. https://doi.org/10.24432/C5KG68

The code in this repository is licensed under the MIT license; see
`LICENSE` for details.

## Experimental cross‑validation and algorithm comparison

Recent advances in aging research suggest that model choice and
hyperparameter tuning greatly influence predictive performance.  In
particular, elastic‑net penalised logistic regression often performs
well relative to other algorithms when features are highly
correlated, while nonlinear models like support‑vector machines and
gradient‑boosted trees can capture complex patterns【348516617573011†L170-L198】.
Supervised learning reviews also emphasise that feature selection,
stratified cross‑validation and multi‑metric evaluation (e.g., AUROC,
AUPRC, accuracy and Brier score) are essential for robust
benchmarking【257543399768406†L260-L329】.

To explore these recommendations, the script `src/train_experiment.py`
implements a configurable experimental pipeline.  Given a processed
dataset and a YAML configuration file, it performs stratified
cross‑validation over multiple algorithms, searches hyperparameters
via grid search using macro‑averaged one‑vs‑rest ROC AUC as the
selection criterion and reports averaged metrics across folds.

By default the experiment compares elastic‑net logistic regression, support‑vector machines, XGBoost, a **random‑forest classifier** and a **multi‑layer perceptron (MLP)** neural network.  A **RandomOverSampler** can be inserted before scaling and classification when the `use_oversampling` flag is enabled in the config.  You can enable or disable models via the `algorithms` list in the YAML.

Run the experiment as follows:

```bash
cd project
python src/train_experiment.py --config configs/base.yaml
```

The `experiment` section of the configuration controls the design:

- `cv_folds`: number of stratified folds (default 5).
  - `algorithms`: list of model names to evaluate (options: `elastic_net_logreg`, `svm`, `xgboost`, `random_forest`, `mlp`).
  - `use_oversampling`: enable a RandomOverSampler for class imbalance.
  - `param_grids`: optional hyperparameter grids overriding the defaults.  A sample grid for `random_forest` is provided in the default `base.yaml`.

After running, results are written to `results/experiment_results.json` and
the best model (based on AUROC) is saved to `results/best_model.joblib`.

## Automated pipeline with runner.py

To streamline experimentation when adding new datasets, the repository
provides a convenience script `src/runner.py`.  This wrapper calls
all the components in sequence: it processes raw data, merges all
processed CSVs, performs cross‑validated experiments, runs feature
selection and generates a performance summary plot.  Use it whenever
you add new raw data files into `data/raw/` or when you want to
re‑execute the entire pipeline from scratch.

To run the automated pipeline, simply execute:

```bash
cd project
python src/runner.py --config configs/base.yaml
```

The runner will create or update `data/processed/processed.csv`, write
models and metrics into `results/` and produce two figures:

- `feature_selection.png` – the top features ranked by an ANOVA F‑test.
- `experiment_summary.png` – a grouped bar chart comparing AUROC,
  AUPRC, accuracy and Brier score for each algorithm.

## Stacking ensemble

While cross‑validation across individual algorithms helps identify the
best single model, combining models can further improve predictive
performance.  The script `src/stacking_ensemble.py` constructs a
stacking classifier that integrates elastic‑net logistic regression,
random forests, XGBoost and a multi‑layer perceptron.  A logistic
regression model serves as the meta‑learner.  The ensemble is
evaluated via stratified cross‑validation and yields an overall
metric summary saved to `results/ensemble_results.json`.  The final
trained ensemble is stored as `results/stacking_model.joblib`.

Run the ensemble training as follows:

```bash
cd project
python src/stacking_ensemble.py --config configs/base.yaml
```

This step is optional but recommended when you have aggregated
multiple datasets or want to push predictive accuracy beyond any
single model.  You can further customise the ensemble by editing
`src/stacking_ensemble.py` to adjust base learners or the meta‑learner.

If multiple processed CSVs are present, the runner uses
`merge_datasets.py` to combine them before training.  This design
makes it easy to expand the dataset with additional yeast lifespan
measurements or gene expression profiles.

To visualize the performance of each algorithm across metrics, use the
helper script `src/visualize_experiment.py`.  It reads the JSON results
and produces a grouped bar chart (saved as `results/experiment_summary.png`):

```bash
python src/visualize_experiment.py --results-file results/experiment_results.json --output-file results/experiment_summary.png
```

### Extending the dataset

The demo currently relies on the UCI Yeast localisation dataset, which has
little direct relevance to ageing.  To make the pipeline more useful for
chronological or replicative lifespan prediction, you can supply additional
raw datasets in `data/raw/` and update `configs/base.yaml` to point to a
merged `processed.csv`.  For example, you might download a publicly
available yeast lifespan dataset or gene expression compendium, convert it
to a CSV with a `survival_label` column and append the new rows to the
existing processed dataset.  A helper script `src/merge_datasets.py` is
provided to merge multiple processed CSVs: run it with the `--input-dir`
argument pointing at a directory of processed files (each containing a
`survival_label` column) and the `--output-file` specifying where to
write the combined dataset.  The cross‑validated experiment script will
automatically adapt to the larger dataset and explore the specified
algorithms.

## Nested cross‑validation

When tuning a large number of hyperparameters or comparing many
algorithms, a single cross‑validation loop can produce optimistic
estimates because the same data are used both to select hyperparameters
and to evaluate performance.  Nested cross‑validation mitigates this
issue by introducing an outer test fold: for each outer fold, a grid
search is run on the training portion using an inner cross‑validation
loop to select hyperparameters, and the resulting best model is
evaluated on the held‑out outer fold.  Averaging over outer folds
provides an unbiased estimate of how the model will perform on new
data.

The script `src/train_nested_cv.py` implements nested cross‑validation for
all algorithms defined in the configuration.  Enable it by adding a
`nested_cv` section to your YAML config:

```yaml
nested_cv:
  enabled: true
  outer_folds: 5
  inner_folds: 3
```

and run:

```bash
python src/train_nested_cv.py --config configs/base.yaml
```

Results will be written to `results/nested_cv_results.json` and the best
model (across both loops) will be saved as `best_model_nested.joblib`.

### Oversampling for imbalanced datasets

Biological datasets often exhibit class imbalance (e.g., rare
phenotypes).  You can instruct the training pipelines to apply a
RandomOverSampler to each fold by setting `experiment.use_oversampling:
true` in the configuration.  Oversampling is applied ahead of feature
scaling to rebalance the classes.  If the optional
`imbalanced‑learn` package is not installed, the oversampling step is
silently skipped.

### Exploratory data analysis (EDA)

Before fitting any model, it is good practice to explore your data.
The `src/eda.py` script computes descriptive statistics for each
numeric feature, draws a correlation heatmap and embeds samples into
two dimensions using UMAP (if available) or t‑SNE.  These plots can
reveal skewed distributions, correlated features, batch effects and
class separability.

```bash
python src/eda.py --config configs/base.yaml
```

This command writes three files into the results directory:

* `eda_stats.json` – JSON summary statistics for each numeric feature.
* `eda_correlation.png` – Heatmap of pairwise Pearson correlations.
* `eda_embedding.png` – 2D scatter plot of UMAP/t‑SNE coloured by the target.

### Automated machine learning (AutoML)

For broader model discovery and hyperparameter optimisation, the script
`src/automl.py` uses the [FLAML](https://github.com/microsoft/FLAML)
AutoML library.  FLAML searches a large space of algorithms and
configurations efficiently.  Set a time budget (in seconds) and run:

```bash
python src/automl.py --config configs/base.yaml --time-budget 300
```

The best model and its parameters are saved to
`results/automl_model.joblib` and `results/automl_best_params.json`.  Note
that FLAML requires the optional `flaml` package listed in
`requirements.txt`.

### Advanced models

Modern bioinformatics leverages a variety of specialised models.  The
module `src/advanced_models.py` contains placeholders for
transformer‑based sequence classifiers, survival analysis via Cox
proportional hazards and graph neural networks.  These models are not
fully implemented but illustrate how one could integrate such
approaches into the pipeline.  Implementing them requires
dependencies like `transformers`, `torch`, and `lifelines`, and
domain‑specific feature engineering.

### Unit tests and continuous integration

To ensure robustness, a minimal test suite resides in the `tests/`
directory.  The tests can be run locally using `pytest` (included in
`requirements.txt`):

```bash
pytest -q
```

A GitHub Actions workflow defined in `.github/workflows/ci.yml` runs
the tests automatically on each push.  This helps catch errors early
when extending the project.

### Docker and reproducible builds

A `Dockerfile` is provided to build a container with all
dependencies pinned to specific versions.  Building the image ensures
that your environment matches the one used for development:

```bash
docker build -t methuselah-prediction .
docker run --rm -v $(pwd):/workspace -w /workspace methuselah-prediction \
  python src/runner.py --config configs/base.yaml
```

### Contributing

We welcome contributions of code, datasets and documentation.  If you
add a new dataset, please include licensing information and update the
configuration file.  For new code, ensure that tests are added and
that the README and requirements are updated accordingly.  See
`CONTRIBUTING.md` for detailed guidelines, including our expectations
regarding coding standards, ethics and bias considerations in ageing
research.